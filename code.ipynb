{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depedencies\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy\n",
    "import random\n",
    "import matplotlib.pyplot as plot\n",
    "from deap.algorithms import eaSimple\n",
    "from deap import base, creator, tools\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading dataset\n",
    "\n",
    "patient_data = pd.read_csv('data.csv')\n",
    "patient_data = patient_data.iloc[: , 1:]\n",
    "print(patient_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing various classification function\n",
    "y = patient_data.iloc[:,23]\n",
    "X = patient_data.iloc[:,:23]\n",
    "colList = list(X.columns)\n",
    "colList[7], colList[1] =  colList[1], colList[7]\n",
    "X = X[colList]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(\"Features:\\n\")\n",
    "print(*X, sep='\\n')\n",
    "\n",
    "def performSVM(x_train, x_test, y_train, y_test):\n",
    "    SVM = svm.LinearSVC(dual=False)\n",
    "    SVM.fit(x_train, y_train)\n",
    "    print(\"Accuracy on SVM:\")\n",
    "    score = SVM.score(x_test, y_test)\n",
    "    print(score)\n",
    "\n",
    "def performLGR(x_train, x_test, y_train, y_test):\n",
    "    lgr = LogisticRegression(max_iter=5000)\n",
    "    lgr.fit(x_train, y_train)\n",
    "    # predictions = lgr.predict(x_test)\n",
    "    score = lgr.score(x_test, y_test)\n",
    "    print(\"Accuracy on LR:\")\n",
    "    print(score)\n",
    "\n",
    "def performKnn(x_train, x_test, y_train, y_test):\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(x_train,y_train)\n",
    "    score = knn.score(x_test, y_test)\n",
    "    print(\"Accuracy on Knn:\")\n",
    "    print(score)\n",
    "    return score\n",
    "\n",
    "def performKnnPred(x_train, x_test, y_train, y_test):\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(x_train,y_train)\n",
    "    score = knn.score(x_test, y_test)\n",
    "    preds = knn.predict(x_test)\n",
    "    print(\"Accuracy on Knn:\")\n",
    "    print(score)\n",
    "    return score, preds\n",
    "\n",
    "def performANN(x_train, x_test, y_train, y_test):\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(23, activation=\"relu\", name=\"layer1\"),\n",
    "            layers.Dense(10, activation=\"relu\", name=\"layer2\"),\n",
    "            layers.Dense(3, activation=\"softmax\",name=\"layer3\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=50)\n",
    "    model.evaluate(x_test, y_test)\n",
    "\n",
    "# function to remove the unselected features from the given array from GA algorithm\n",
    "def reduceFeatures(features, selectionArr):\n",
    "    selected_features = features\n",
    "    for i in selectionArr:\n",
    "        if i == 0:\n",
    "            selected_features = selected_features.drop(selected_features.columns[[i]], axis=1) \n",
    "    return selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X\n",
    "target = y\n",
    "\n",
    "# split the data into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, stratify=target\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "\n",
    "def fitness_score(individual):\n",
    "    column_support = pd.Series(individual).astype(bool)\n",
    "    global x_train, y_train, x_test, y_test, model\n",
    "    \n",
    "    x_train_ = x_train[x_train.columns[column_support]]\n",
    "    x_test_ = x_test[x_test.columns[column_support]]\n",
    "\n",
    "    model.fit(x_train_, y_train)\n",
    "    y_pred = model.predict(x_test_)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return score,\n",
    "\n",
    "n_genes = x_train.shape[1]\n",
    "n_generations = 10\n",
    "n_population = 20\n",
    "crossover_probability = 0.7\n",
    "mutation_probability = 0.2\n",
    "\n",
    "def setup_toolbox():\n",
    "    # container for individuals\n",
    "    creator.create('FitnessMax', base.Fitness, weights=(1.0,))\n",
    "    creator.create('Individual', list, fitness=creator.FitnessMax)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\n",
    "        'individual_generator_function',\n",
    "        random.randint, 0, 1 \n",
    "    )\n",
    "    # method to populate individual\n",
    "    toolbox.register(\n",
    "        'individual_generator',\n",
    "        tools.initRepeat,\n",
    "        creator.Individual,\n",
    "        toolbox.individual_generator_function,\n",
    "        n_genes\n",
    "    )\n",
    "    # method to create population\n",
    "    toolbox.register(\n",
    "        'population_generator',\n",
    "        tools.initRepeat,\n",
    "        list,\n",
    "        toolbox.individual_generator\n",
    "    )\n",
    "    # fitness calculation\n",
    "    toolbox.register(\n",
    "        'evaluate', fitness_score\n",
    "    )\n",
    "    # selection\n",
    "    toolbox.register(\n",
    "        'select', tools.selRoulette\n",
    "    )\n",
    "    # crossover\n",
    "    toolbox.register('mate', tools.cxOnePoint)\n",
    "    # mutation\n",
    "    toolbox.register(\n",
    "        'mutate',\n",
    "        tools.mutFlipBit,\n",
    "        indpb=mutation_probability\n",
    "    )\n",
    "    return toolbox\n",
    "\n",
    "# setup deap toolbox\n",
    "toolbox = setup_toolbox()\n",
    "\n",
    "# create a population\n",
    "population = toolbox.population_generator(n_population)\n",
    "\n",
    "# A simple evolutionary algorithm\n",
    "final_feature_population, logbook = eaSimple(\n",
    "    population, \n",
    "    toolbox, \n",
    "    crossover_probability, \n",
    "    mutation_probability, \n",
    "    n_generations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_amount_feat = []\n",
    "feat_count = 20\n",
    "\n",
    "for n in final_feature_population:\n",
    "    # score = compute_fitness_score(n)\n",
    "    selected_feats = reduceFeatures(features, n)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(selected_feats, target, test_size=0.2)\n",
    "    score = performKnn(x_train, x_test, y_train, y_test)\n",
    "    print(n)\n",
    "    currrent_feat_count = list(n).count(1)\n",
    "    print(currrent_feat_count)\n",
    "    print(score)\n",
    "    if score > 0.9:\n",
    "        if currrent_feat_count < feat_count:\n",
    "            least_amount_feat = n\n",
    "            feat_count = currrent_feat_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking the reduced features with best fitness score from round 1\n",
    "print(\"Selected feature array\")\n",
    "print(least_amount_feat)\n",
    "count = list(least_amount_feat).count(1)\n",
    "print(count)\n",
    "\n",
    "# picking selected features from the original feature table\n",
    "selected_feats = reduceFeatures(features, least_amount_feat)\n",
    "x_train, x_test, y_train, y_test = train_test_split(selected_feats, target, test_size=0.2)\n",
    "\n",
    "print(\"Reduced Features (Round 1):\\n\")\n",
    "print(*selected_feats, sep='\\n')\n",
    "\n",
    "# kNN on reduced features\n",
    "score, preds = performKnnPred(x_train, x_test, y_train, y_test)\n",
    "print(confusion_matrix(y_test, preds))\n",
    "\n",
    "# SVM on reduced features\n",
    "performSVM(x_train, x_test, y_train, y_test)\n",
    "\n",
    "# LGR on reduced features\n",
    "performLGR(x_train, x_test, y_train, y_test)\n",
    "\n",
    "# ANN on reduced features\n",
    "performANN(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = selected_feats\n",
    "target = y\n",
    "\n",
    "# split the data into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, stratify=target\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "\n",
    "def fitness_function(individual):\n",
    "    \"\"\"\n",
    "    Select the features from the individual, train\n",
    "    and compute the accuracy_score.\n",
    "    \n",
    "    Example:\n",
    "    individual = [0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1]\n",
    "    The 1 represents the presence of features and\n",
    "    0 represents the absence of features\n",
    "    \n",
    "    \"\"\"\n",
    "    column_support = pd.Series(individual).astype(bool)\n",
    "    global x_train, y_train, x_test, y_test, model\n",
    "    \n",
    "    x_train_ = x_train[x_train.columns[column_support]]\n",
    "    x_test_ = x_test[x_test.columns[column_support]]\n",
    "\n",
    "    model.fit(x_train_, y_train)\n",
    "    y_pred = model.predict(x_test_)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return score,\n",
    "\n",
    "n_genes = x_train.shape[1]\n",
    "n_generations = 10\n",
    "n_population = 8\n",
    "crossover_probab = 0.7\n",
    "mutation_probab = 0.2\n",
    "\n",
    "def setup_toolbox():\n",
    "    # container for individuals\n",
    "    creator.create('FitnessMax', base.Fitness, weights=(1.0,))\n",
    "    creator.create('Individual', list, fitness=creator.FitnessMax)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\n",
    "        'individual_generator_function',\n",
    "        random.randint, 0, 1 \n",
    "    )\n",
    "    # method to populate individual\n",
    "    toolbox.register(\n",
    "        'individual_generator',\n",
    "        tools.initRepeat,\n",
    "        creator.Individual,\n",
    "        toolbox.individual_generator_function,\n",
    "        n_genes\n",
    "    )\n",
    "    # method to create population\n",
    "    toolbox.register(\n",
    "        'population_generator',\n",
    "        tools.initRepeat,\n",
    "        list,\n",
    "        toolbox.individual_generator\n",
    "    )\n",
    "    # fitness score calculation\n",
    "    toolbox.register(\n",
    "        'evaluate', fitness_function\n",
    "    )\n",
    "    # selection\n",
    "    toolbox.register(\n",
    "        'select', tools.selRoulette\n",
    "    )\n",
    "    # crossover\n",
    "    toolbox.register('mate', tools.cxOnePoint)\n",
    "    # mutation\n",
    "    toolbox.register(\n",
    "        'mutate',\n",
    "        tools.mutFlipBit,\n",
    "        indpb=mutation_probab\n",
    "    )\n",
    "    return toolbox\n",
    "\n",
    "# setup deap toolbox\n",
    "toolbox = setup_toolbox()\n",
    "\n",
    "# create a population\n",
    "population = toolbox.population_generator(n_population)\n",
    "\n",
    "# A simple evolutionary algorithm\n",
    "final_feature_population, logbook = eaSimple(\n",
    "    population, \n",
    "    toolbox, \n",
    "    crossover_probab, \n",
    "    mutation_probab, \n",
    "    n_generations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking the best feature set from the round 2 of GA with feature count equal to 5\n",
    "least_amount_feat = []\n",
    "feat_count = 8\n",
    "\n",
    "for feature in final_feature_population:\n",
    "    # score = compute_fitness_score(n)\n",
    "    new_selected_feats = reduceFeatures(selected_feats, feature)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(new_selected_feats, target, test_size=0.2)\n",
    "    score = performKnn(x_train, x_test, y_train, y_test)\n",
    "    print(feature)\n",
    "    currrent_feat_count = list(feature).count(1)\n",
    "    print(currrent_feat_count)\n",
    "    print(score)\n",
    "    if score > 0.9:\n",
    "        if currrent_feat_count == 5:\n",
    "            least_amount_feat = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the accuracy and performance metrices on features selected from second round of GA\n",
    "print(\"Selected feature array\")\n",
    "print(least_amount_feat)\n",
    "count = list(least_amount_feat).count(1)\n",
    "print(count)\n",
    "\n",
    "# picking selected features from the original feature table\n",
    "selected_feats = reduceFeatures(features, least_amount_feat)\n",
    "x_train, x_test, y_train, y_test = train_test_split(selected_feats, target, test_size=0.2)\n",
    "\n",
    "print(\"Reduced Features (Round 2):\\n\")\n",
    "print(*selected_feats, sep='\\n')\n",
    "\n",
    "# kNN on reduced features\n",
    "score, preds = performKnnPred(x_train, x_test, y_train, y_test)\n",
    "print(confusion_matrix(y_test, preds))\n",
    "\n",
    "print(f1_score(y_test, preds, average=\"macro\"))\n",
    "print(precision_score(y_test, preds, average=\"macro\"))\n",
    "print(recall_score(y_test, preds, average=\"macro\")) \n",
    "\n",
    "# SVM on reduced features\n",
    "performSVM(x_train, x_test, y_train, y_test)\n",
    "\n",
    "# LGR on reduced features\n",
    "performLGR(x_train, x_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
